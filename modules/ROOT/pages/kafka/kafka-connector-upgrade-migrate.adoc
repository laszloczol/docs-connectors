= Kafka - Upgrade and Migrate - Mule 4
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:imagesdir: ../../assets/images/

Upgrade Anypoint Connector for Kafka (Kafka Connector) to version 4.0.0

== Supported Upgrade Paths

[%header,cols="50a,50a"]
|===
|From Version | To Version
|3.2.x |4.0.0
|===

== Changes in This Release

This release contains the following changes:

* New operations, with new parameters. (see in the: New Operations, Parameters table)

* The Message Listener replaces the Message Consumer Source, it has new parameters are shown in the New Source Table. (see in the: Changed Source table)

* New Source was added (see in the: New Source table)

* Added different Ack modes in the Consumer Configuration AUTO: Mule ACKs the message only if the flow is finished successfully; MANUAL: The user must do the ack manually within the flow; DUPS_OK: Same as the AUTO mode, but the commit is made asynchronously, which can lead to duplicate records; IMMEDIATE: Mule automatically ACKs the message upon reception.

* Added Transactional SASL connection, this connection type is the same as the Producer SASL connection, but with the difference that it’s not a cached connection and is transactional.

* Added Transactional plaintext connection, this connection type is the same as the Producer plaintext connection, but with the difference that it’s not a cached connection and is transactional.

== Changed Source, Old version, Parameters, and Return Types

[%header%autowidth.spread]
|===
|Kafka Source | Old version | Parameters

| Message listener | Message consumer | ** Poll timeout; ** Poll timeout time unit; ** Acknowledgement mode; ** Amount of parallel consumers

|===

== New Operations, Parameters, and Return Types

[%header%autowidth.spread]
|===
|Kafka Operation | Description | Parameters

| Commit
a| Commits the offsets associated to a message or batch of messages consumed in a Message Listener. |
** Consumer commit key; ** (The consumer commit key to use to commit the message.)

| Consume |This operation allows receiving messages from one or more Kafka topics, it works very similarly to the Message Listener source, so all the operations that apply to that, will apply to this operation as well. |
** Consumption timeout; **

** Timeout time unit; **

|Describe | Retrieves the subscriptions/assignments of the associated connection, along with the current offsets for each topic-partition. This operation will have to block all the Consumers, so there are no changes to the offsets assignments or other information, that will be retrieved by this operation. |

| Publish | The publish operation will allow sending messages to a Kafka topic.It will be non blocking, and will it’s behavior will depend on specific values of the connection parameters.
            This operation will support transactions, for this, a transaction id will be randomly generated for the KafkaProducer, which will be handled in the Connection.
| ** Topic; ** (The topic name)

** Partition;** (The partition name)

** Key; **

** Message; **

** Headers; **
| Seek | Sets the current offset of the consumer for the given topic and partition to the provided offset value. |

** Topic; ** (The topic name)

** Partition; ** (The partition name)

** Offset; ** (The offset to seek to) |

|===


== New Source, Parameters, and Return Types

[%header%autowidth.spread]
|===
|Kafka Operation | Description | Parameters

| Batch message listener
a|  Works in the same way as the Message Listener with the difference that it will process the list of messages instead of a single message at a time. As the message list that was obtained in the poll will be handled by a flow as a single event, the handling of concurrency should be simpler than in the “Simple” Message Listener, e.g.: commit of the messages happens for all the messages as a whole (simply calling the commit() on KC.
| ** Poll timeout; ** Poll timeout time unit; ** Acknowledgement mode; ** Amount of parallel consumers;
|===

== Changes in Operations Metadata

This release contains the following changes related to keys, input metadata, and output metadata


== Upgrade Prerequisites

Before you perform the upgrade, you must complete the following:

. Create a backup of your files, data, and configuration in case you need to restore to the previous version.

== Upgrade Steps

Follow these steps to perform the upgrade to Kafka Connector 4.0.0:

.. In Studio, create a Mule project.
.. In the Mule Palette view, click *Search in Exchange*.
.. In Add Modules to Project, type 'Kafka' in the search field.
.. In Available modules, select *Kafka Connector*, and then click *Add*.
.. Click *Finish*.
.. Verify that the `kafka-connector` dependency version is `4.0.0` in the pom.xml.

Anypoint Studio upgrades the connector automatically.


== Post Upgrade Steps

After you install the latest version of the connector, follow these steps to complete the upgrade:


. In Anypoint Studio, verify that there are no errors in the *Problems* or *Console* views.
. Check the project pom.xml and verify that there are no problems.
. Test the connection and verify that the operations work.


== Troubleshooting

If there are problems with caching the parameters and caching the metadata, try restarting Anypoint Studio.

=== Reverting the Upgrade

If it is necessary to revert to the previous version of Kafka Connector, change the `kafka-connector` dependency version `4.0.0` in the project's pom.xml to the previous version.

You must update the project's pom.xml in Anypoint Studio.

== See Also

https://help.mulesoft.com[MuleSoft Help Center]
